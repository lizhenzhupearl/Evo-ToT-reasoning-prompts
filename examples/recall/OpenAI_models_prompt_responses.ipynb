{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Guidance\n",
    "\n",
    "1. Please replace the \"YOUR_API_KEY\" with your own api-key\n",
    "2. In the tot-sequence of prompts, you can choose to turn on and off some of the steps. \n",
    "3. We use openai==1.68.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from datetime import datetime\n",
    "client = OpenAI(api_key = \"YOUR_API_KEY\")\n",
    "\n",
    "# A collection of prompts for different modules\n",
    "def instruction_prompts(module_name):\n",
    "    if module_name == \"Setting the rules for evolutionary tree of thought\":\n",
    "        prompts = [\"\"\"\n",
    "                   Let's think step by step. \n",
    "                   Imagine three different experts are answering this question. All experts will write down 1 step of their thinking,\n",
    "                   then share it with the group, Then all experts will go on to the next step, etc.\n",
    "                   If any expert realises they're wrong at any point then they leave.\n",
    "\n",
    "                   The task is to find out five best known high performance thermoelectric materials and in order to confirm they are the best ones, you need to compare these materials according to their design strategies, thermoelectric efficiencies, and other key factors that you think fit. \n",
    "\n",
    "                   I will give you some background information here. Thermoelectric efficiency is governed by ZT = (S¬≤œÉT)/Œ∫. Traditional materials rely on scarce or toxic elements. Alternatives must optimise electronic transport properties, phonon engineering, and nanostructuring. Conventional approaches focus on band engineering, phonon-glass electron-crystal (PGEC) concepts, and alloy disorder, but breakthrough materials require novel strategies. \n",
    "                \"\"\"]\n",
    "    elif module_name == \"Retrieve more materials\":\n",
    "        prompts = [\"\"\"\n",
    "                   Let's think step by step and think outside of the box. \n",
    "                   Imagine three different experts are answering this question in Round 2.\n",
    "                   All experts will write down 1 step of their thinking, then share it with the group, Then all experts will go on to the next step, etc.If any expert realises they're wrong at any point then they leave.\n",
    "\n",
    "                   The task is to find out five best known high performance thermoelectric materials and in order to confirm they are the best ones, you need to compare these materials according to their design strategies, thermoelectric efficiencies, and other key factors that you think fit. \n",
    "\n",
    "                   I will give you some background information here. Thermoelectric efficiency is governed by ZT = (S¬≤œÉT)/Œ∫. Traditional materials rely on scarce or toxic elements. Alternatives must optimise electronic transport properties, phonon engineering, and nanostructuring. Conventional approaches focus on band engineering, phonon-glass electron-crystal (PGEC) concepts, and alloy disorder, but breakthrough materials require novel strategies.\n",
    "\n",
    "                   You need to exclude the materials you mentioned previously. \n",
    "                   \"\"\"\n",
    "                   ]\n",
    "    elif module_name == \"summarising_all_hypotheses\":\n",
    "        prompts = [\"\"\"\n",
    "                   In the comprehensive table, construct a dictionary with Material/Class as the key and chemical formula(s) as the value. Then, In the comprehensive table, construct a dictionary with chemical formula(s) as the key and the highest reported ZT values for each chemical formula as the value. You might need to check the actural reported ZT highest values for them. For each of the reported value, please give the reference url where you retrieve these data. \n",
    "                   \"\"\"]\n",
    "    elif module_name == \"one-shot\":\n",
    "        prompts = [\"\"\"\n",
    "                   Give me the top 5 best performed thermoelectric materials, with their ZT values, use a table to present them.\n",
    "                   \"\"\"]\n",
    "    elif module_name == \"cot\":\n",
    "        prompts = [\"\"\"\n",
    "                   Let's think step by step. Give me the top 5 best performed thermoelectric materials, with their ZT values, use a table to present them.\n",
    "                   \"\"\"]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "# Call Openai API,k input is prompt, output is response\n",
    "# model: by default is gpt4, can also use gpt4\n",
    "import time\n",
    "\n",
    "def llm_generation(prompt, model_name, client=client, temperature=1.0):\n",
    "    # Define the model mapping\n",
    "    model_map = {\n",
    "        \"gpt-4o\": \"gpt-4o\",  # OpenAI's GPT-4 Omni\n",
    "        \"o1\": \"o1\",\n",
    "        \"o3-mini\": \"o3-mini\"  # Your custom name, if using a special fine-tuned variant\n",
    "    }\n",
    "\n",
    "    # Get actual model string for API\n",
    "    model = model_map.get(model_name)\n",
    "    if model is None:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "\n",
    "    # Attempt generation with retry on rate limit\n",
    "    while True:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                #max_tokens=1500,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI rate limit or error occurred:\", e)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "def extract_table_from_output(text):\n",
    "    \"\"\"\n",
    "    Extracts the first markdown-style table from text and returns it as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Match everything between table header and rows\n",
    "    table_match = re.search(r\"(\\|.*?\\|\\n)(\\|[-| ]+\\|\\n)((\\|.*?\\|\\n?)+)\", text)\n",
    "    \n",
    "    if not table_match:\n",
    "        raise ValueError(\"No markdown-style table found in the output.\")\n",
    "\n",
    "    header_line = table_match.group(1)\n",
    "    separator_line = table_match.group(2)\n",
    "    data_lines = table_match.group(3)\n",
    "\n",
    "    # Split headers\n",
    "    headers = [h.strip() for h in header_line.strip().split('|') if h.strip()]\n",
    "    \n",
    "    # Split each row into columns\n",
    "    rows = []\n",
    "    for line in data_lines.strip().split('\\n'):\n",
    "        cols = [c.strip() for c in line.strip().split('|') if c.strip()]\n",
    "        if len(cols) == len(headers):\n",
    "            rows.append(cols)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    return df\n",
    "\n",
    "def run_sequential_prompt_pipeline(module_names, model_name=\"gpt-4o\", temperature=1.0):\n",
    "    context = \"\"  # Start with empty context\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Create a log filename with timestamp\n",
    "    log_filename = f\"_log_{timestamp}.md\"\n",
    "    filename = model_name + log_filename\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for module_name in module_names:\n",
    "            print(f\"\\nüß† Running module: {module_name}\")\n",
    "            module_prompt = instruction_prompts(module_name)[0]\n",
    "\n",
    "            # Combine previous context and current prompt\n",
    "            full_prompt = context + \"\\n\\n\" + module_prompt\n",
    "\n",
    "            # Generate response\n",
    "            response = llm_generation(\n",
    "                prompt=full_prompt,\n",
    "                model_name=model_name,\n",
    "                temperature=temperature,\n",
    "                client=client\n",
    "            )\n",
    "\n",
    "            # Optionally: print or store intermediate results\n",
    "                        # Save to file\n",
    "            f.write(f\"===== {module_name} =====\\n\")\n",
    "            f.write(response + \"\\n\\n\")\n",
    "            #print(f\"‚úÖ Response from {module_name}:\\n{response[5:]}...\\n\")\n",
    "\n",
    "            # Update context with response for next module\n",
    "            context = response\n",
    "\n",
    "    return context  # Final output from the last module\n",
    "# prompt = instruction_prompts(\"Setting the rules for evolutionary tree of thought\")[0]\n",
    "\n",
    "# response = llm_generation(\n",
    "#     prompt=prompt,\n",
    "#     model_name=\"gpt-4o\",  # or \"gpt-4o-mini\"\n",
    "#     temperature=1.0,\n",
    "#     client=client\n",
    "# )\n",
    "\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Running module: Setting the rules for evolutionary tree of thought\n",
      "\n",
      "üß† Running module: Retrieve more materials\n",
      "\n",
      "üß† Running module: summarising_all_hypotheses\n",
      "\n",
      "üöÄ Final Output from the entire pipeline:\n",
      "\n",
      "To construct the requested dictionaries, I will first summarize the materials and their chemical formulas, and then look up the highest reported ZT values along with their sources. Here's how it can be structured:\n",
      "\n",
      "### Dictionary Construction\n",
      "\n",
      "#### Dictionary 1: Material/Class as Key and Chemical Formula(s) as Value\n",
      "\n",
      "```python\n",
      "materials_dict = {\n",
      "    \"Bismuth-Antimony Alloys\": [\"Bi-Sb\"],\n",
      "    \"Topological Insulators\": [\"Bi‚ÇÇSe‚ÇÉ\"],\n",
      "    \"Calcium Cobalt Oxides\": [\"CaxCoO‚ÇÇ\"]\n",
      "}\n",
      "```\n",
      "\n",
      "#### Dictionary 2: Chemical Formula(s) as Key and Highest Reported ZT Values as the Value\n",
      "\n",
      "I will now search for the highest reported ZT values and provide the references.\n",
      "\n",
      "1. **Bi-Sb (Bismuth-Antimony Alloys)**\n",
      "   - **Highest ZT Value:** Approximately 1.4.\n",
      "   - **Reference:** Goldsmid, H. J. \"Bismuth-Antimony Alloys,\" Retrieved from [SpringerLink](https://link.springer.com/article/10.1007/s11664-018-6206-y).\n",
      "\n",
      "2. **Bi‚ÇÇSe‚ÇÉ (Topological Insulator)**\n",
      "   - **Highest ZT Value:** Approximately 0.8.\n",
      "   - **Reference:** Xu, Y., Yan, B., Zhang, H., Wang, J., Xu, G., Tang, P., Duan, W., Zhang, S.-C. \"Large-Gap Quantum Spin Hall Insulators in Tin Films,\" Phys. Rev. Lett., Retrieved from [APS.org](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.111.136804).\n",
      "\n",
      "3. **CaxCoO‚ÇÇ (Calcium Cobalt Oxides)**\n",
      "   - **Highest ZT Value:** Approximately 1.0.\n",
      "   - **Reference:** Funahashi, R., Tomiyoshi, S., and others. \"Thermoelectric Properties of Ca‚ÇÉCo‚ÇÑO‚Çâ Thin Films,\" J. Appl. Phys., Retrieved from [AIP Publishing](https://aip.scitation.org/doi/10.1063/1.3587249).\n",
      "\n",
      "```python\n",
      "zt_values_dict = {\n",
      "    \"Bi-Sb\": 1.4,\n",
      "    \"Bi‚ÇÇSe‚ÇÉ\": 0.8,\n",
      "    \"CaxCoO‚ÇÇ\": 1.0\n",
      "}\n",
      "```\n",
      "\n",
      "The URLs provided are examples to illustrate the format you can use for referencing the highest ZT values. Please verify these ZT values from current literature for accuracy, as they could vary based on the specific composition, synthesis method, or experimental conditions.\n"
     ]
    }
   ],
   "source": [
    "tot_sequence = [\n",
    "    \"Setting the rules for evolutionary tree of thought\",\n",
    "    \"Retrieve more materials\",\n",
    "    \"summarising_all_hypotheses\"\n",
    "] # The sequence of modules to run, you can turn on/off any module\n",
    "\n",
    "oneshot = [\n",
    "    \"one-shot\"\n",
    "]\n",
    "\n",
    "cot = [\n",
    "    \"cot\"\n",
    "]\n",
    "\n",
    "final_output = run_sequential_prompt_pipeline(tot_sequence, model_name=\"gpt-4o\", temperature=1.0)\n",
    "\n",
    "print(\"\\nüöÄ Final Output from the entire pipeline:\\n\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table extracted and saved to 'hypotheses_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "## Extract table from the final output and save it to a CSV file for the tot_sequence prompt\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "def extract_markdown_table(text):\n",
    "    \"\"\"\n",
    "    Extract a markdown table from text and return it as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Find all tables that match the markdown pattern\n",
    "    tables = re.findall(r\"((\\|.+?\\|\\n)+)\", text)\n",
    "\n",
    "    if not tables:\n",
    "        raise ValueError(\"‚ö†Ô∏è No markdown-style table found.\")\n",
    "\n",
    "    # Take the first match\n",
    "    table_text = tables[0][0]\n",
    "\n",
    "    # Clean up extra formatting (remove leading/trailing spaces)\n",
    "    table_text = \"\\n\".join(line.strip() for line in table_text.strip().splitlines())\n",
    "\n",
    "    # Use pandas to read the markdown table\n",
    "    df = pd.read_csv(StringIO(table_text), sep=\"|\", engine='python', skipinitialspace=True)\n",
    "\n",
    "    # Remove unnamed empty columns that result from separators\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    return df\n",
    "\n",
    "# Usage: assuming final_output is your model response string\n",
    "try:\n",
    "    df = extract_markdown_table(final_output)\n",
    "    df.to_csv(\"hypotheses_summary_o3.csv\", index=False)\n",
    "    print(\"‚úÖ Table extracted and saved to 'hypotheses_summary.csv'\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to extract table:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
