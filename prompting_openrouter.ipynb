{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Guidance\n",
    "\n",
    "1. Please replace the \"YOUR_API_KEY\" with your own api-key\n",
    "2. In the tot-sequence of prompts, you can choose to turn on and off some of the steps. \n",
    "3. We use openai==1.68.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "OpenRouter_API_key = 'YOUR_API_KEY'  # Replace with your OpenRouter API key\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OpenRouter_API_key,\n",
    ")\n",
    "\n",
    "# A collection of prompts for different modules\n",
    "def instruction_prompts(module_name):\n",
    "    if module_name == \"Setting the rules for evolutionary tree of thought\":\n",
    "        prompts = [\"\"\"\n",
    "                Imagine three different experts are answering this question. \n",
    "                All experts will write down 1 step of their thinking, then share it with the group, the group will do the following operations: \n",
    "                1. figure out the key inspirations and strategies, by creating a list of them; \n",
    "                2. revisiting the list, using operations such as mutation, crossover to generate new inspirations and strategies; \n",
    "                3. then each expert will have an updated hypothesis, however, this hypothesis might not be rationale or might be impossible; \n",
    "                therefore, 4. their new hypotheses should be checked before going to the next step, if valid, accept, if not valid, try to generate a new one, until it becomes valid. \n",
    "                Sometimes, an objective, if exist could be used to help the three experts. After this round of hypothesis generation, then all experts will go on to the next step, etc. If any expert realises they're wrong at any point then they will point it out and correct it. \n",
    "                \n",
    "                Following the above action instruction, now the three experts are solving the problem: Generate three novel and valid hypotheses to improve the performance of thermoelectric materials or design entirely new ones based on a research background and literature-inspired insights. \n",
    "                For the proposed thermoelectric materials, the potential objective is the figure of merit ZT values. This ZT values could also be used by the experts as one factor to help refine their hypotheses. \n",
    "                \n",
    "                After the experts generated the hypotheses, for each hypothesis, it has to pass the two stages checking. \n",
    "                Stage 1 is Decision Tree workflow: \n",
    "                [Step 1] Does the hypothesis follow known chemical and physical rules?]  NO ‚Üí REJECT (Not physically meaningful)  YES ‚Üí Step 2; \n",
    "                [Step 2] Does it introduce a novel thermoelectric strategy?  NO ‚Üí MODIFY (Refine concept)  YES ‚Üí Step 3;  \n",
    "                [Step 3] Is it computationally testable (DFT, MD, phonon transport)?  NO ‚Üí MODIFY (Improve predictive validation)  YES ‚Üí Step 4  \n",
    "                [Step 4] Is the material synthetically feasible?  NO ‚Üí MODIFY (Propose better synthesis route)  YES ‚Üí go to Step 5  \n",
    "                [Step 5] Does it show high potential for improving ZT?  NO ‚Üí REJECT (Low thermoelectric impact)  YES ‚Üí go to Step 6  \n",
    "                [Step 6] Does it have a reasonable risk-reward balance?  NO ‚Üí MODIFY (Address high risks)  YES ‚Üí ACCEPT for further computational and experimental testing  \n",
    "                \n",
    "                Stage 2 is the scoring framework: Once a hypothesis passes the decision tree, it is ranked numerically based on five key evaluation criteria.\n",
    "                | Criteria     | Description                                              | Score Range |\n",
    "                |--------------|----------------------------------------------------------|-------------|\n",
    "                | Validness    | Consistency with physical & chemical principles          | 1-10        |\n",
    "                | Novelty      | Innovation compared to known thermoelectrics             | 1-10        |\n",
    "                | Significance | Impact on ZT or new mechanisms                           | 1-10        |\n",
    "                | Feasibility  | Ease of computational modeling and synthesis             | 1-10        |\n",
    "                | Risk         | Likelihood of failure due to instability or complexity   | 1-10        |\n",
    "\n",
    "                **Total Score Calculation**  \n",
    "                Total Score=Validness+Novelty+Significance+Feasibility-Risk\n",
    "                Interpretation of Scores:\n",
    "                45-50: Top priority hypothesis for immediate testing\n",
    "                35-44: Promising but requires some refinements\n",
    "                25-34: Moderate, needs major improvements before testing\n",
    "                <25: Rejected or needs complete reformulation\n",
    "                \"\"\"]\n",
    "    elif module_name == \"check_evolution\":\n",
    "        prompts = [\"\"\"\n",
    "                   Now the three experts need to campare the hypothesis before and after the Mutation and Crossover. They need to check if they have better hypotheses after these operations. \n",
    "                   If yes, we accept them, if not, new hypotheses need to be formed.\n",
    "                   \"\"\"\n",
    "                   ]\n",
    "    elif module_name == \"generating hypothesis with chemical formulas\":\n",
    "        prompts = [\"\"\"\n",
    "                   Based on the above inspirations, the three experts are proposing new thermoelectric materials that haven't been reported before step by step. I need to know the composition of them for example their chemical formulas. They should validate your options and comment on the feasibility of the proposed materials. If you are asked to proposed a new material and try to write down the composition or chemical formula of it. Bear in mind, how many chemical rules should we check in order to make sure that the proposed new materials is chemically feasible or chemically allowed and what are the steps to follow? You also need to consider that materials have a wide range of branches, such as layered materials, high entropy alloys or other organic inorganic compounded materials, therefore, the chemical rules for these materials should also be considered. \n",
    "                   \"\"\"]\n",
    "    elif module_name == \"novelty evaluation\":\n",
    "        prompts = [\"\"\"\n",
    "                   Let's have a double check on the hypothesis. Are they entirely new? If not, we need to propose new ones. Are they better over the other methods? If not, we need to use a new strategy. After checking, we also need to present the predicted ZT values and working temperatures of them. \n",
    "                   \"\"\"]              \n",
    "    elif module_name == \"context guidance\":\n",
    "        prompts = [\"\"\"\n",
    "                   Actually, the experts are given some background research informations to help them think: How can new material design strategies enhance the thermoelectric efficiency of emerging materials beyond traditional semiconductors? Context: Thermoelectric efficiency is governed by ZT = (S¬≤œÉT)/Œ∫. Traditional materials (e.g., Bi‚ÇÇTe‚ÇÉ, PbTe) rely on scarce or toxic elements. Alternatives must optimise electronic transport properties, phonon engineering, and nanostructuring. Conventional approaches focus on band engineering, phonon-glass electron-crystal (PGEC) concepts, and alloy disorder, but breakthrough materials require novel strategies. \n",
    "                   So based on these, the experts start a second round of problem solving.\n",
    "                   \"\"\"]\n",
    "    elif module_name == \"compare_hypotheses\":\n",
    "        prompts = [\"\"\"\n",
    "                   Now, at this stage, let's compare and double check that the three experts are getting optimised hypotheses or not by looking at: 1. compare current hypotheses with the three hypotheses from first round of discussion; 2. a comparative analysis of the three experts‚Äô hypotheses. If both are true, we can accept these hypotheses. If not, the experts need to find new ones by starting a new round of problem solving.\n",
    "                   \"\"\"]\n",
    "    elif module_name == \"summarising_hypothesis\":\n",
    "        prompts = [\"\"\"\n",
    "                   Now the three experts can generate a hypothesis with an example template: \"We hypothesise that [strategy] can enhance thermoelectric efficiency via [mechanism], inspired by [insight]. This will be tested by [method], however the risk lies in [Risk].\" \n",
    "                   And summarise and organise the hypotheses in the table contains the columns with Material, Formula,\tStructure Type, Hypothesis,Predicted ZT, Operating Temperature (K), extend the table with evaluation Rate 1-10 for: 1. **Validity:** Consistent with known physics? 2. **Novelty:** Unconventional concept? 3. **Significance:** Meaningful advance? 4. **Feasibility:** Experimentally/computationally testable? 5. **Risk:** potential risk of being failure? Similar to the lone pair electrons in perovskite materials, which makes them standing out in photovoltaic materials and contributing to the defect tolerance; what are the killer properties/effects, that you think will make these hypothesized thermoelectric materials stand out? Include these \"killer effects\" in the table.\n",
    "                   Refine hypothesis accordingly.\n",
    "                   \"\"\"]\n",
    "    elif module_name == \"removing_low_feasibility_hypotheses\":\n",
    "        prompts = [\"\"\"\n",
    "                   Now the three experts are thinking to test the hypothesis in practice. If the feasibility is low and risk is high, the three experts need to filter the current hypothesis to keep the good ones. and because we need three hypotheses, now the three experts need to find out another hypothesis by another round of problem solving. Bear in mind to check the hypothesis checking workflow and keep the experts discussing following the procedures.\n",
    "                   \"\"\"]\n",
    "    elif module_name == \"zoom_into_low_temperature_hypotheses\":\n",
    "        prompts = [\"\"\"\n",
    "                   Now the three experts are zooming into another challenge for thermoelectrics to find the thermoelectric materials working especially well at low-temperature (lower than 600 K) a lot, will you still give the same recommended hypothesis, or you will have different ones? Give me your updated summary of hypotheses. Bear in mind to check the hypothesis checking workflow and keep the experts discussing following the procedures.\n",
    "                   \"\"\"]\n",
    "    elif module_name == \"summarising_all_hypotheses\":\n",
    "        prompts = [\"\"\"\n",
    "                   Now summarise all the hypotheses proposed from the very beginning to current ones, so I can easily see the trends and what has been changed. Give me the clean Markdown TABLE (styled: | col | col |) that contains the columns with Material, Formula,   Structure Type, Hypothesis,Predicted ZT, Operating Temperature (K), extend the table with evaluation Rate 1-10 for: 1. **Validity:** Consistent with known physics? 2. **Novelty:** Unconventional concept? 3. **Significance:** Meaningful advance? 4. **Feasibility:** Experimentally/computationally testable? 5. **Risk:** potential risk of being failure? Similar to the lone pair electrons in perovskite materials, which makes them standing out in photovoltaic materials and contributing to the defect tolerance; what are the killer properties/effects, that you think will make these hypothesized thermoelectric materials stand out? Include these \"killer effects\" in the table; finally label if the hypothesis is kept or discarded.\n",
    "                   \"\"\"]\n",
    "    elif module_name == \"one-shot\":\n",
    "        prompts = [\"\"\"\n",
    "                   What is the next promising thermoelectric material, that no one has never reported before? I'd like to have a try. What is the composition of it? Is this material never reported before? Why you propose this one? What is your inspiration? \n",
    "                   \"\"\"]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "# Call Openai API,k input is prompt, output is response\n",
    "# model: by default is gpt4, can also use gpt4\n",
    "\n",
    "def llm_openai_generation(prompt, model_name, client=client, temperature=1.0):\n",
    "    # Define the model mapping\n",
    "    model_map = {\n",
    "        \"4o\": \"gpt-4o\",  # T=0-2 \n",
    "        \"o1\": \"o1\", #T = 1\n",
    "        \"o3-mini\": \"o3-mini\" , # T=1\n",
    "        \"mistral\" : \"mistralai/mistral-large-2407\", #T 0-3.2\n",
    "        \"claude\" : \"anthropic/claude-3.7-sonnet\", #T 0-1\n",
    "        \"gemini\" :\"google/gemini-pro-1.5\",#\"google/gemini-2.0-pro-exp-02-05:free\",#, \"google/gemini-2.5-pro-exp-03-25:free\", #T 0-2, topK, topP, slow response\n",
    "        \"deepseek\":\"deepseek/deepseek-chat-v3-0324\", #\"deepseek/deepseek-r1:free\", # T 0-1.5, slow response\n",
    "        \"llama\":\"meta-llama/llama-3.1-405b-instruct\"\n",
    "    }\n",
    "\n",
    "    # Get actual model string for API\n",
    "    model = model_map.get(model_name)\n",
    "    if model is None:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "    if model_name == \"gemini\":\n",
    "        completion = client.chat.completions.create(\n",
    "            extra_headers={\n",
    "                \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n",
    "                \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n",
    "            },\n",
    "            #temperature=temperature,\n",
    "            #topP = 0.95,\n",
    "            extra_body={},\n",
    "            model=model,\n",
    "            messages=[\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\":  prompt\n",
    "                        }]\n",
    "                    }\n",
    "                #{\"role\": \"user\", \"content\": [{\"type\": \"text\",\"text\": prompt}]}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    else:\n",
    "        # Attempt generation with retry on rate limit\n",
    "        while True:\n",
    "            try:\n",
    "                completion = client.chat.completions.create(\n",
    "                    extra_headers={\n",
    "                        \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n",
    "                        \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n",
    "                    },\n",
    "                    temperature=temperature,\n",
    "                    extra_body={},\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "                # completion = client.chat.completions.create(\n",
    "                #     model=model,\n",
    "                #     temperature=temperature,\n",
    "                #     #max_tokens=1500,\n",
    "                #     messages=[\n",
    "                #         {\"role\": \"user\", \"content\": prompt}\n",
    "                #     ]\n",
    "                # )\n",
    "                return completion.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(\"OpenAI rate limit or error occurred:\", e)\n",
    "                time.sleep(60)\n",
    "\n",
    "def extract_table_from_output(text):\n",
    "    \"\"\"\n",
    "    Extracts the first markdown-style table from text and returns it as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Match everything between table header and rows\n",
    "    table_match = re.search(r\"(\\|.*?\\|\\n)(\\|[-| ]+\\|\\n)((\\|.*?\\|\\n?)+)\", text)\n",
    "    \n",
    "    if not table_match:\n",
    "        raise ValueError(\"No markdown-style table found in the output.\")\n",
    "\n",
    "    header_line = table_match.group(1)\n",
    "    separator_line = table_match.group(2)\n",
    "    data_lines = table_match.group(3)\n",
    "\n",
    "    # Split headers\n",
    "    headers = [h.strip() for h in header_line.strip().split('|') if h.strip()]\n",
    "    \n",
    "    # Split each row into columns\n",
    "    rows = []\n",
    "    for line in data_lines.strip().split('\\n'):\n",
    "        cols = [c.strip() for c in line.strip().split('|') if c.strip()]\n",
    "        if len(cols) == len(headers):\n",
    "            rows.append(cols)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    return df\n",
    "\n",
    "def run_sequential_prompt_pipeline(module_names, model_name=\"gpt-4o\", temperature=1.0):\n",
    "    context = \"\"  # Start with empty context\n",
    "    # Generate current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Create a log filename with timestamp\n",
    "    log_filename = f\"_log_{timestamp}.md\"\n",
    "    filename = model_name + log_filename\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for module_name in module_names:\n",
    "            print(f\"\\nüß† Running module: {module_name}\")\n",
    "            module_prompt = instruction_prompts(module_name)[0]\n",
    "\n",
    "            # Combine previous context and current prompt\n",
    "            full_prompt = context + \"\\n\\n\" + module_prompt\n",
    "\n",
    "            # Generate response\n",
    "            while True:\n",
    "                try:\n",
    "                    response = llm_openai_generation(\n",
    "                        prompt=full_prompt,\n",
    "                        model_name=model_name,\n",
    "                        temperature=temperature,\n",
    "                        client=client\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"try failed\")\n",
    "                    print(\"OpenAI rate limit or error occurred:\", e)\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                break\n",
    "                    \n",
    "            # response = llm_openai_generation(\n",
    "            #     prompt=full_prompt,\n",
    "            #     model_name=model_name,\n",
    "            #     temperature=temperature,\n",
    "            #     client=client\n",
    "            # )\n",
    "\n",
    "            # Optionally: print or store intermediate results\n",
    "                        # Save to file\n",
    "            f.write(f\"===== {module_name} =====\\n\")\n",
    "            f.write(response + \"\\n\\n\")\n",
    "            #print(f\"‚úÖ Response from {module_name}:\\n{response[5:]}...\\n\")\n",
    "\n",
    "            # Update context with response for next module\n",
    "            context = response\n",
    "\n",
    "    return context  # Final output from the last module\n",
    "# prompt = instruction_prompts(\"Setting the rules for evolutionary tree of thought\")[0]\n",
    "\n",
    "# response = llm_generation(\n",
    "#     prompt=prompt,\n",
    "#     model_name=\"gpt-4o\",  # or \"gpt-4o-mini\"\n",
    "#     temperature=1.0,\n",
    "#     client=client\n",
    "# )\n",
    "\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sequence = [\n",
    "    \"Setting the rules for evolutionary tree of thought\",\n",
    "    \"check_evolution\",\n",
    "    \"generating hypothesis with chemical formulas\",\n",
    "    \"novelty evaluation\",\n",
    "    \"context guidance\",\n",
    "    \"compare_hypotheses\",\n",
    "    \"summarising_hypothesis\",\n",
    "    \"removing_low_feasibility_hypotheses\",\n",
    "    \"zoom_into_low_temperature_hypotheses\",\n",
    "    \"summarising_all_hypotheses\"\n",
    "] # The sequence of modules to run, you can turn on/off any module\n",
    "\n",
    "oneshot = [\n",
    "    \"one-shot\"\n",
    "]\n",
    "\n",
    "#final_output = run_sequential_prompt_pipeline(oneshot, model_name=\"4o\", temperature=1.0)\n",
    "#final_output = run_sequential_prompt_pipeline(oneshot, model_name=\"o1\", temperature=1.0)\n",
    "final_output = run_sequential_prompt_pipeline(oneshot, model_name=\"gemini\", temperature=1.0)\n",
    "#final_output = run_sequential_prompt_pipeline(tot_sequence, model_name=\"o1\", temperature=1.0)\n",
    "\n",
    "print(\"\\nüöÄ Final Output from the entire pipeline:\\n\")\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract table from the final output and save it to a CSV file for the tot_sequence prompt\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "def extract_markdown_table(text):\n",
    "    \"\"\"\n",
    "    Extract a markdown table from text and return it as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Find all tables that match the markdown pattern\n",
    "    tables = re.findall(r\"((\\|.+?\\|\\n)+)\", text)\n",
    "\n",
    "    if not tables:\n",
    "        raise ValueError(\"‚ö†Ô∏è No markdown-style table found.\")\n",
    "\n",
    "    # Take the first match\n",
    "    table_text = tables[0][0]\n",
    "\n",
    "    # Clean up extra formatting (remove leading/trailing spaces)\n",
    "    table_text = \"\\n\".join(line.strip() for line in table_text.strip().splitlines())\n",
    "\n",
    "    # Use pandas to read the markdown table\n",
    "    df = pd.read_csv(StringIO(table_text), sep=\"|\", engine='python', skipinitialspace=True)\n",
    "\n",
    "    # Remove unnamed empty columns that result from separators\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    return df\n",
    "\n",
    "# Usage: assuming final_output is your model response string\n",
    "try:\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    df = extract_markdown_table(final_output)\n",
    "    df.to_csv(\"hypotheses_summary_\"+timestamp+\".csv\", index=False)\n",
    "    print(\"‚úÖ Table extracted and saved to 'hypotheses_summary.csv'\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to extract table:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding-based novelty (SentenceTransformers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def read_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file if line.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Baseline hypotheses\n",
    "base_file_path_one = '/logs/4o_log_3.md'\n",
    "baseline_hypotheses_one = read_sentences(base_file_path_one)\n",
    "base_file_path_tot = 'logs/gpt-4o_log_3.md'\n",
    "baseline_hypotheses_tot = read_sentences(base_file_path_tot)\n",
    "baseline_embeddings = model.encode(baseline_hypotheses_one, convert_to_tensor=True)\n",
    "baseline_embeddings_tot = model.encode(baseline_hypotheses_tot, convert_to_tensor=True)\n",
    "\n",
    "# Hypotheses from the final output\n",
    "#read all the files have .md in the folder\n",
    "import os\n",
    "import glob\n",
    "folder_path = '/logs/'\n",
    "files = glob.glob(folder_path + '*.md')\n",
    "#if the file first line contains '===== Setting the rules for evolutionary tree of thought =====', then read the file as tot_hypotheses\n",
    "for f in files:\n",
    "    with open(f, 'r', encoding='utf-8') as file:\n",
    "        first_line = file.readline().strip()\n",
    "        if first_line == '===== Setting the rules for evolutionary tree of thought =====':\n",
    "            tot_hypotheses = read_sentences(f)\n",
    "            hypothesis_embedding = model.encode(tot_hypotheses, convert_to_tensor=True)\n",
    "            # Mean cosine distance to baseline hypotheses (higher = more novel)\n",
    "            novelty_score = 1 - util.cos_sim(hypothesis_embedding, baseline_embeddings_tot).mean().item()\n",
    "            print(f\"Novelty score: {novelty_score:.2f}\")\n",
    "            #save the file name and novelty score in a csv file as two columns\n",
    "            with open('novelty_score_tot3.csv', 'a') as file:\n",
    "                file.write(f + ',' + str(novelty_score) + '\\n')\n",
    "        else:\n",
    "            one_hypotheses = read_sentences(f)\n",
    "            hypotheses_embedding = model.encode(one_hypotheses, convert_to_tensor=True)\n",
    "            # Mean cosine distance to baseline hypotheses (higher = more novel)\n",
    "            novelty_score = 1 - util.cos_sim(hypotheses_embedding, baseline_embeddings).mean().item()\n",
    "            print(f\"Novelty score: {novelty_score:.2f}\")\n",
    "            #save the file name and novelty score in a csv file as two columns\n",
    "            with open('novelty_score_one3.csv', 'a') as file:\n",
    "                file.write(f + ',' + str(novelty_score) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
